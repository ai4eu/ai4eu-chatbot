conda create -n "ai4eu-chat" python=3.7.10

#enable the conda environment (this has a fixed path in my workstation)
source ./scripts/conda-activate.sh

# We need version 0.17.0 since this supports all the latest transformers
pip install deeppavlov

# We need torch for the models. 
# In FORTH we have an AMD Radeon VII gpu and we are using the rocm enable pip package
# Download torch from https://download.pytorch.org/whl/rocm4.0.1/torch
# https://download.pytorch.org/whl/rocm4.0.1/torch-1.9.0%2Brocm4.0.1-cp37-cp37m-linux_x86_64.whl
pip install torch-1.9.0+rocm4.0.1-cp37-cp37m-linux_x86_64.whl

# Also for RTX 30x cards please use the following for tensorflow (needs though a python 3.8 environment)
# This is not yet tested. I have not managed to run tensorflow in albireo
pip install --upgrade pip
pip install nvidia-pyindex
pip install nvidia-tensorflow[horovod]
pip install nvidia-tensorboard==1.15


# REST service  - sanic
pip3 install sanic

# Install hugginface and sentence-transformers
# Newer version seem to be problematic. Have to check why though
# With 0.16.0 of deeppavlov we can support the latest transformers library
pip install transformers sentence-transformers

# deeppavlov supports numpy=0.18.0

# To train/evaluate/interact our model

Install kbqa_cq related packages
python -m deeppavlov install kbqa_cq


python -m deeppavlov train config/...
python -m deeppavlov evaluate config/...
python -m deeppavlov interact config/...

Add also -d to the above when running for the first time, so that we can download any needed pretrained models

# How to run
nohup python src/RESTQA.py &

# A simple request
curl -X POST "https://pangaia.ics.forth.gr/ai4eu-chatbot/" -d "{\"query\":\"What is AI4EU?\"}"




# Models used

=> Intents Classifier
   model: all-mpnet-base-v2.json
   For training please remove from the out the y_pred_probas

   python -m deeppavlov train  config/intents/sentence-emb/all-mpnet-base-v2.json

=> Sentence Embeddings
   model: bert-sentence_multi_cased_L-12_H-768_A-12_pt.json
   This is a model provided by deeppavlov

   We just use the pretrained model -> No need for training


=> Slot-filler just uses a fuzzy slot mapping approach
   We could probably try to use an entity based approach in the future


=> FAQ: It is a classifer over pairs of QA 
   model: all-mpnet-base-v2.json (default)


   python -m deeppavlov train  config/faq/sentence-emb/all-mpnet-base-v2.json
