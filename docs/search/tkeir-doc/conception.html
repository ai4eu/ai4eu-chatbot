

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Conception &mdash; T-KEIR 1.0.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="installation.html" />
    <link rel="prev" title="Overview" href="overview.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> T-KEIR
          

          
            
            <img src="_static/T-KEIR_logo_FIN-02.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Conception</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#data-preparation">Data preparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#construction-of-terminological-lists">Construction of terminological lists</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparation-of-the-evaluation-data">Preparation of the evaluation data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#document-analysis">Document Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tokenization">Tokenization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#using-regular-expressions">Using regular expressions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#grouping-of-detached-compound-words">Grouping of detached-compound words</a></li>
<li class="toctree-l4"><a class="reference internal" href="#resource-usage">Resource usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#normalization-rule">Normalization rule</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#morphosyntax">Morphosyntax</a></li>
<li class="toctree-l3"><a class="reference internal" href="#named-entities-extraction">Named entities extraction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#principles">Principles</a></li>
<li class="toctree-l4"><a class="reference internal" href="#use-validation-rules">Use validation rules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#dependencies-analysis-triple-subject-verb-object-extraction">Dependencies analysis &amp; triple &lt;Subject, Verb, Object&gt; extraction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">Principles</a></li>
<li class="toctree-l4"><a class="reference internal" href="#syntactic-rules">Syntactic rules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#keywords-extraction">Keywords extraction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#classifier">Classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="#summarizer">Summarizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#relation-clustering">Relation clustering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#question-and-answering">Question and Answering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#indexing">Indexing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#index-design">Index design</a></li>
<li class="toctree-l3"><a class="reference internal" href="#from-documentary-analysis-to-index">From documentary analysis to index</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#searching">Searching</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#analysis-of-the-request">Analysis of the request</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-elasticsearch-queries">Building Elasticsearch queries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#standard-queries">Standard queries</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#query-expansion">Query Expansion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#combine-re-ranking-of-results">Combine &amp; Re-Ranking of results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#combination-strategies">Combination strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scoring-of-results">Scoring of results</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ready_to_run.html">Create a ready to run application with docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools/tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="security.html">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="oam.html">Operation and Management</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">T-KEIR</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Conception</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/conception.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="conception">
<h1>Conception<a class="headerlink" href="#conception" title="Permalink to this headline">¶</a></h1>
<img alt="_images/conception1.png" src="_images/conception1.png" />
<div class="section" id="data-preparation">
<h2>Data preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline">¶</a></h2>
<p>Data preparation consists in</p>
<ul class="simple">
<li><p>transform the documents into a format adapted to the tools,</p></li>
<li><p>build terminological and structured resources (such as ontology concepts for example),</p></li>
<li><p>construct the evaluation data.</p></li>
</ul>
<img alt="_images/conception2.png" src="_images/conception2.png" />
<div class="section" id="construction-of-terminological-lists">
<h3>Construction of terminological lists<a class="headerlink" href="#construction-of-terminological-lists" title="Permalink to this headline">¶</a></h3>
<p>Linguistic resources are used by document analysis tools to extract data typed in the target domain
as well as generic data such as city names (to improve the detection of named entities).</p>
</div>
<div class="section" id="preparation-of-the-evaluation-data">
<h3>Preparation of the evaluation data<a class="headerlink" href="#preparation-of-the-evaluation-data" title="Permalink to this headline">¶</a></h3>
<p>The evaluation data are constructed to know the relevance of the results returned by the
search system. We seek to have a set of queries associated with the relevant documents to be returned.
The goal is ultimately to assess the capacity of the search engine</p>
</div>
</div>
<div class="section" id="document-analysis">
<h2>Document Analysis<a class="headerlink" href="#document-analysis" title="Permalink to this headline">¶</a></h2>
<img alt="_images/TheresisNLP.png" src="_images/TheresisNLP.png" />
<div class="section" id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">¶</a></h3>
<p>The tokenization phase allows a text to be segmented into linguistic units: sentences, phrases, words.</p>
<img alt="_images/conception3.png" src="_images/conception3.png" />
<p>Principles
Segmentation is a delicate phase that requires the use of regular expressions and strategies to group compound word.</p>
<div class="section" id="using-regular-expressions">
<h4>Using regular expressions<a class="headerlink" href="#using-regular-expressions" title="Permalink to this headline">¶</a></h4>
<p>Regular expressions allow you to define segmentation rules. These rules cover, among other things:</p>
<ul class="simple">
<li><p>The fact that the ‘.’ Is not systematically used as the end of a sentence, in the case of a decimal number in English
for example where the period is a separator</p></li>
<li><p>The fact that the ‘-‘ at the beginning at the end of words is separated</p></li>
<li><p>The fact that the ‘-‘ in the middle of a word is not segmented</p></li>
<li><p>…</p></li>
</ul>
</div>
<div class="section" id="grouping-of-detached-compound-words">
<h4>Grouping of detached-compound words<a class="headerlink" href="#grouping-of-detached-compound-words" title="Permalink to this headline">¶</a></h4>
<p>Detached compound words often represent semantic units, for example the sequence “hot dog”
should be taken as a phrase and should not be segmented into two words (“hot”, “dog”).
This problem is addressed in T-KEIR tools through the use of phrase list and a Trie
type tree data structure.</p>
</div>
<div class="section" id="resource-usage">
<h4>Resource usage<a class="headerlink" href="#resource-usage" title="Permalink to this headline">¶</a></h4>
<p>Linguistic resources provide a list of phrases that can be typed (for example, the list
of city names in the geoname database is labeled as a place). They can also define a notion
of hierarchy in the case of an ontology of concepts. All of these resources are “compiled”
into a Trie structure. This data structure can be configured to remove diacritics
(add-ascii-folding option), to add a morphosyntaxic label (pos option) or a named entity
label (label option)</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><img alt="_images/annotation.png" src="_images/annotation.png" />
</td>
</tr>
<tr class="row-even"><td><p>Example of resource configuration file</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><img alt="_images/example-resource.png" src="_images/example-resource.png" />
</td>
</tr>
<tr class="row-even"><td><p>Example of resource file</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="normalization-rule">
<h4>Normalization rule<a class="headerlink" href="#normalization-rule" title="Permalink to this headline">¶</a></h4>
<p>T-KEIR tools provide the ability to normalize words and perform spell checking of the most
common mistakes. Here simple transformation rules are set up by means of configuration files.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><img alt="_images/example-typo.png" src="_images/example-typo.png" />
</td>
</tr>
<tr class="row-even"><td><p>Example of typo configuration file</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><img alt="_images/example-normalization.png" src="_images/example-normalization.png" />
</td>
</tr>
<tr class="row-even"><td><p>Example of normalization configuration file</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="morphosyntax">
<h3>Morphosyntax<a class="headerlink" href="#morphosyntax" title="Permalink to this headline">¶</a></h3>
<p>The morphosyntax module is built on the Spacy library. It provides the possibility of giving
each segmented term during the tokenization phase a morphosyntactic label (noun, verb, adjective, …).
This module takes advantage of the pre-tagged information from the segmentation phase by “forcing”
the tags of phrases and terminology often unrecognized by the original morphosyntactic tagger.
It is also this module which provides the lemmatized form (this is the canonical form of a word,
for example the verb “doing” has the lemma “do”) of words.</p>
<img alt="_images/example-ms.png" src="_images/example-ms.png" />
</div>
<div class="section" id="named-entities-extraction">
<h3>Named entities extraction<a class="headerlink" href="#named-entities-extraction" title="Permalink to this headline">¶</a></h3>
<img alt="_images/conception5.png" src="_images/conception5.png" />
<dl class="simple">
<dt>Named feature extraction involves labeling textual elements. They can be seen as &lt;text, label&gt; pairs</dt><dd><p>where the label is the type of data, for example “city”, “person”, “organization”, …</p>
</dd>
</dl>
<div class="section" id="principles">
<h4>Principles<a class="headerlink" href="#principles" title="Permalink to this headline">¶</a></h4>
<p>The tagger implemented in the use case uses the Spacy library and uses the elements
extracted during the segmentation phase as well as validation rules built with the
morphosyntactic elements.</p>
</div>
<div class="section" id="use-validation-rules">
<h4>Use validation rules<a class="headerlink" href="#use-validation-rules" title="Permalink to this headline">¶</a></h4>
<p>Validation rules help to avoid basic errors such as associating a city name with a verb.</p>
<img alt="_images/validation-rules.png" src="_images/validation-rules.png" />
</div>
</div>
<div class="section" id="dependencies-analysis-triple-subject-verb-object-extraction">
<h3>Dependencies analysis &amp; triple &lt;Subject, Verb, Object&gt; extraction<a class="headerlink" href="#dependencies-analysis-triple-subject-verb-object-extraction" title="Permalink to this headline">¶</a></h3>
<img alt="_images/conception4.png" src="_images/conception4.png" />
<p>Dependency analysis allows the discovery of relationships between the different structuring
elements of a sentence.
It therefore provides the possibility of creating &lt;Subject, Verb, Object&gt; triples which will
form the basis of a knowledge graph automatically constructed by T-KEIR.</p>
<div class="section" id="id1">
<h4>Principles<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>The Dependency Analyzer relies on the Spacy library to extract syntactic dependencies.
This analysis is improved by taking advantage of the groupings carried out during the
previous phases (segmentation, morphosyntax and extraction of named entities). Thus the
structured elements detected by Spacy are extended with the data from the previous phases.
Finally, the &lt;Subject, Verb, Object&gt; triples are extracted using syntactic patterns defined
in a configuration file.</p>
</div>
<div class="section" id="syntactic-rules">
<h4>Syntactic rules<a class="headerlink" href="#syntactic-rules" title="Permalink to this headline">¶</a></h4>
<p>The syntactic rules allow the definition of patterns corresponding to phrases, verbal groups
or prepositional groups. The creation of these rules is governed by the syntax defined in the
Spacy library.</p>
</div>
</div>
<div class="section" id="keywords-extraction">
<h3>Keywords extraction<a class="headerlink" href="#keywords-extraction" title="Permalink to this headline">¶</a></h3>
<p>The keywords are the most relevant words or sequences of words in a document. When they are
weighted, they allow, for example, the creation of word clouds.
Extracting them is a good way to naively summarize a document by pointing to the most relevant
elements.</p>
<p>To judge the relevance of the different terms we used the Rake algorithm. It is built on the
observation that keywords are found between empty words and punctuation marks. The algorithm
extracts and weights these word sequences using a method described in “Rose, Stuart &amp; Engel,
Dave &amp; Cramer, Nick &amp; Cowley, Wendy. (2010). Automatic Keyword Extraction from Individual Documents.
10.1002 / 9780470689646.ch1 (Automatic Keyword Extraction from Individual Documents (researchgate.net))”.
T-KEIR uses a modified version of Rake taking into account lemmatized forms and their
morphosyntaxic tags. Thus empty words will be associated with the labels of determinants and
other conjunctions while the delimiters will be associated with the punctuation tags.</p>
</div>
<div class="section" id="classifier">
<h3>Classifier<a class="headerlink" href="#classifier" title="Permalink to this headline">¶</a></h3>
<img alt="_images/conception8.png" src="_images/conception8.png" />
<p>The unsupervised classifer allow to classify text for a given (by the user) set of class / label.</p>
</div>
<div class="section" id="summarizer">
<h3>Summarizer<a class="headerlink" href="#summarizer" title="Permalink to this headline">¶</a></h3>
<img alt="_images/conception9.png" src="_images/conception9.png" />
<p>The summarizer propose an automatic and extractive summmary of document.</p>
</div>
<div class="section" id="relation-clustering">
<h3>Relation clustering<a class="headerlink" href="#relation-clustering" title="Permalink to this headline">¶</a></h3>
<img alt="_images/conception6.png" src="_images/conception6.png" />
<p>The relations clustering allows to associate a semantic class with the elements of
&lt;Subject, Verb, Object&gt; triples (and keywords) in an unsupervised manner (without human intervention).</p>
<p>The construction of the classes is carried out in two stages:</p>
<ul class="simple">
<li><p>The elements of the &lt;Subject, Verb, Object&gt; triples and the keywords are vectorized using a
transformer neural network (here we use the LaBSE transformer from the Huggingface library)
pre-trained on a large amount of data covering a number varied fields. Each of these vectors can
be seen as an embedding of sequences of words (associated with a Subject, a Verb, an Object or
a keyword) in a semantic space created by the Transformer.</p></li>
<li><p>These four sets of vectors (associated respectively with the Subject, Verb, Object and Keywords)
are clustered by a clusterin algorithm. From then on, each vector is assigned a
class number and the algorithm creates a model to predict the class of a new vector.</p></li>
</ul>
<p>Using a clustering algorithm is a good way to do semantic quantization: we don’t store the vectors,
only their semantic classes predicted by the clustering model.</p>
</div>
</div>
<div class="section" id="question-and-answering">
<h2>Question and Answering<a class="headerlink" href="#question-and-answering" title="Permalink to this headline">¶</a></h2>
<img alt="_images/conception7.png" src="_images/conception7.png" />
</div>
<div class="section" id="indexing">
<h2>Indexing<a class="headerlink" href="#indexing" title="Permalink to this headline">¶</a></h2>
<div class="section" id="index-design">
<h3>Index design<a class="headerlink" href="#index-design" title="Permalink to this headline">¶</a></h3>
<p>The index built by the T-KEIR library is the subject of a particular design responding to the various
functionalities which are:</p>
<ul class="simple">
<li><p>Standard information search (by key words / phrase).</p></li>
<li><p>Structured research by using a knowledge graph with access to Subject, Verb / Predicate / Property, Object type
triples. It is essential to note here, that beyond the standard search, the index has been constructed in
such a way that the business concepts extracted from the ontology can be used as search criteria. They are
therefore considered as &lt;Subject, Predicate, Object&gt; triples of a knowledge graph.</p></li>
</ul>
<table class="docutils align-default">
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><img alt="_images/index-design-hl.png" src="_images/index-design-hl.png" />
</td>
</tr>
<tr class="row-even"><td><p>The index is built of several facets:</p>
<ul class="simple">
<li><p>title: document title</p></li>
<li><p>content: content of the document</p></li>
<li><p>lemma_title: lemmatized version of the title where
the tool words and punctuation have been removed</p></li>
<li><p>lemma_content: lemmatized version of the content
(without tool words)</p></li>
<li><p>data_source / index_document: pointer to document</p></li>
<li><p>text_suggester: list of keywords allowing the completion
of a query (in the case of a “keywords” type query).</p></li>
<li><p>kg: knowledge graph, a focus is made thereafter.</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><img alt="_images/index-design-ll.png" src="_images/index-design-ll.png" />
</td>
</tr>
<tr class="row-even"><td><p>The kg field contains the basic structure for building a
knowledge graph: the &lt;Subject, Predicate / Property, Object&gt;
triples.
In the T-KEIR index each element of this triplet includes
the following fields:</p>
<ul class="simple">
<li><p>class: the class resulting from the cluster of relations
or keywords when it is available. It is a cluster identifier
linked to the model calculated during the relationship
clustering phase</p></li>
<li><p>content / lemma_content: the textual content of the element</p></li>
<li><p>label: the label associated with the content, in the case of
a named entity this will for example be “place” or “person”</p></li>
<li><p>positions: the positions of the element when they are
available</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="from-documentary-analysis-to-index">
<h3>From documentary analysis to index<a class="headerlink" href="#from-documentary-analysis-to-index" title="Permalink to this headline">¶</a></h3>
<p>The index is fill with results of the linguistic analysis. Each document is analyzed: it follows the diagram
<strong>Documentary analysis</strong>. The indexing tool uses the content of the result of this analysis to generate indexing
ElasticSearch queries following the scheme defined in the Design section of the index.</p>
</div>
</div>
<div class="section" id="searching">
<h2>Searching<a class="headerlink" href="#searching" title="Permalink to this headline">¶</a></h2>
<p>The document searching is the step of querying indexes. The rich structure of the index offers many possibilities
for “querying”. Developing a query involves analyzing the user query, constructing an Elasticsearch query and
manipulating the results</p>
<div class="section" id="analysis-of-the-request">
<h3>Analysis of the request<a class="headerlink" href="#analysis-of-the-request" title="Permalink to this headline">¶</a></h3>
<p>Query analysis follows the same process as document analysis. In this case study, the query corresponds to all or
part of a document and not simply to a sentence or a juxtaposition of keywords. Using an identical documentary
analysis ensures that we have the same documentary enrichment as that carried out for the indexed documents.
The construction of the query to Elasticsearch will therefore be easier.</p>
</div>
<div class="section" id="building-elasticsearch-queries">
<h3>Building Elasticsearch queries<a class="headerlink" href="#building-elasticsearch-queries" title="Permalink to this headline">¶</a></h3>
<p>Elasticsearch provides a very advanced query language (DSL: Domain Specific Language). This language makes
it possible to carry out multifaceted interrogation by integrating notions of “boosting” of query elements
(to give more weight to these elements), advanced combinations of Boolean clauses (OR and AND), notions of
“slop” to manage the alignments between two sequences …</p>
<div class="section" id="standard-queries">
<h4>Standard queries<a class="headerlink" href="#standard-queries" title="Permalink to this headline">¶</a></h4>
<p>Standard querying simply uses a bag of words constructed from the content of the document (the tokenization phase).
This bag of words is sorted by how often the words appear in the document.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><img alt="_images/basic-query.png" src="_images/basic-query.png" />
</td>
</tr>
<tr class="row-even"><td><p>It is possible to configure the request with 2 criteria:
* we use a bag of words sorted by frequency of</p>
<blockquote>
<div><p>occurrence without weighting (uniq-word-query)</p>
</div></blockquote>
<ul class="simple">
<li><p>Either we “boost” each word according to their frequency.</p></li>
</ul>
<p>In both cases, a maximum number of words must be defined
in the query (cut-query option).</p>
</td>
</tr>
</tbody>
</table>
<p>“Advanced” queries allow you to create complex queries based on all fields of the index.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><img alt="_images/advanced-query.png" src="_images/advanced-query.png" />
</td>
</tr>
<tr class="row-even"><td><p>The created query can take into account
* Fields prefixed by “<a href="#id2"><span class="problematic" id="id3">lemma_</span></a>” (use-lemma option)
* Keywords by comparing the keywords from the query with</p>
<blockquote>
<div><p>those extracted during the indexing phase
(use-keyword options)</p>
</div></blockquote>
<ul class="simple">
<li><p>The knowledge graph by building disjunctive queries or the
sub-queries will be conjunctions of the elements of the
triples &lt;Subject, Property / Verb / Predicat, Object / Value&gt;
(use-knowledge-graph option)</p></li>
<li><p>The semantics of the keywords with the classes of the
clusters: in the same way as the relations, the keywords are
the object of a clustering and have at this tritre of classes
which one can interrogate (option use-semantic-keyword )</p></li>
<li><p>The semantics of &lt;Subject, Verb, Object&gt; triples: each element
of the triplet has semantic classes resulting from clustering
(use-semantic-knowledge-graph option)</p></li>
<li><p>The concepts of ontology (those extracted by Linguamatics)</p></li>
<li><p>Sentences in documents.</p></li>
</ul>
<p>Then the types of queries defined above can be refined by configuring
them (querying field). Thus it is possible to define the slop
(maximum distance between the words of each sequence), the boosting
value, etc.</p>
</td>
</tr>
</tbody>
</table>
<p>The use of &lt;Subject, Verb, Subject&gt; triples and sentences leads to the construction
of potentially very large queries. They are all the greater in that to increase the
relevance of the results we combine three forms of query: OR, AND and ExactMatch.</p>
<p>To limit the size of the requests, we apply a clustering algorithm (HDBSCAN) to a TF.IDF
type vectorization of the sequences (document sentences, elements of triples). The clusters
thus created, we only use the most relevant sequences (those close to the centers of the clusters).</p>
</div>
</div>
<div class="section" id="query-expansion">
<h3>Query Expansion<a class="headerlink" href="#query-expansion" title="Permalink to this headline">¶</a></h3>
<p>In the study case, a query expansion option by document was implemented. Here we are looking to
extend the query using other patents.
The strategy implemented is to use the term vectors provided by ElasticsSearch. These vectors contain
for each document the list of terms of the document. Each term contains statistics related to the index:
how often the term appears in the document, throughout the index, and the number of documents in which
the term appears. The idea is to combine all the vectors so as to build a query taking into account all
the documents: query and document to be extended with their associated statistics.</p>
</div>
<div class="section" id="combine-re-ranking-of-results">
<h3>Combine &amp; Re-Ranking of results<a class="headerlink" href="#combine-re-ranking-of-results" title="Permalink to this headline">¶</a></h3>
<img alt="_images/scoring.png" src="_images/scoring.png" />
<div class="section" id="combination-strategies">
<h4>Combination strategies<a class="headerlink" href="#combination-strategies" title="Permalink to this headline">¶</a></h4>
<p>When creating queries we saw that it was possible to take into account several options: use-keyword,
use-concepts, use-knowledge-graph, use-semantic-keywords, use-semanic-knowledgre graph; these different
options can be combined within the same query in which case ElasticSearch will sum the values of the similarities
of each option or else be executed in several sub-queries and in this case the merge is done a posteriori.
In this last strategy the lists of results of each subquery are combined by sum of the scores. To ensure that
there are as many common documents as possible between the search results of the different sub-queries, it is
possible to extend the number of documents returned by ElasticSearch (expand-results option).
The configuration of the choice between merging within a single query or after the execution of several queries
is given by the run-clause-separately field.</p>
</div>
<div class="section" id="scoring-of-results">
<h4>Scoring of results<a class="headerlink" href="#scoring-of-results" title="Permalink to this headline">¶</a></h4>
<p>The result scoring is generally presented to the user, unfortunately the score provided by ElasticSearch is not
bounded and is difficult to interpret as it is. So we have implemented several re-scoring strategies
* By score normalization: the score is normalized by the score of the first ordered document  (the highest score returned by ElasticSearch). The interpretation of the score is simple, corresponding  to the degree of relevance to the most relevant document. But This implies that the most relevant document  will always have a score of 1, even if the actual relevance is low.
* Using metrics on the intersection between the terms of the term vectors of the request and those of the document:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>by-query-size</strong>: we calculate the size of the intersection between a document and the query which we divide  by the size of the query. The score should reflect the presence of all the words of the query in the document.  This is the default configuration.</p></li>
<li><p><strong>by-document-size</strong>: we calculate the size of the intersection between a document and the query which we
divide by the size of the document. The score should reflect a maximum match between the query and the document.</p></li>
<li><dl class="simple">
<dt><strong>by-union-size</strong>: we calculate the size of the intersection between a document and the query which we divide</dt><dd><p>by the size of the union between the query and the document (Jaccard similarity). The score must reflect a
maximum correspondence between the document and the query and penalize the elements of the query (or document)
that do not intersect.</p>
</dd>
</dl>
</li>
<li><p><strong>no-normalization</strong>: no normalization. The score will be the one provided by Elasticsearch.</p></li>
</ul>
</div></blockquote>
<p>The two types of normalization are multiplied to obtain the final score</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="installation.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="overview.html" class="btn btn-neutral float-left" title="Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, THALES SIX GTS FRANCE.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>